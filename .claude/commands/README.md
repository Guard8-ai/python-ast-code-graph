# Claude Code Slash Commands

Custom slash commands for intelligent Python codebase analysis using AST integration mapping.

## Available Commands

### `/analyze-python`
Analyze the Python codebase using AST-based integration mapping and load insights into working memory.

**What it does:**
- Runs integration mapper in context-aware mode (61-70% token reduction)
- Generates compact analysis (72.9K tokens for 900-file Django)
- Loads crossroads and critical paths into memory
- Enables instant architecture queries

**When to use:**
- Start of session: Get codebase overview
- Before complex refactoring: Understand current architecture
- When joining a new project: Learn the structure

**Usage:**
```
/analyze-python
```

**Time to complete:**
- Small (10-50 files): 1-5 seconds
- Medium (50-200 files): 5-15 seconds
- Large (200-1000 files): 15-45 seconds

### `/refresh-analysis`
Refresh the codebase analysis to reflect recent code changes.

**What it does:**
- Re-runs integration mapper
- Compares with previous analysis
- Reports what changed (new files, crossroads, etc.)
- Updates working memory

**When to use:**
- After major refactoring
- After adding new modules
- After merging feature branches
- Periodically (weekly for active projects)

**Usage:**
```
/refresh-analysis
```

## How It Works

### Memory Management Strategy

The commands use a **smart memory approach**:

```
Total context: 200,000 tokens
â”œâ”€ Analysis file: ~70K tokens (35%)
â”‚  â””â”€ Stored at: .claude/codebase_analysis.json
â”œâ”€ Working memory: ~20K tokens (10%)
â”‚  â”œâ”€ Crossroads summary (high-value junctions)
â”‚  â”œâ”€ Critical paths (data flow entry points)
â”‚  â”œâ”€ Architecture overview
â”‚  â””â”€ Sparse component index
â””â”€ Available for work: ~110K tokens (55%)
```

### What Gets Memorized

**High Priority (Always memorize):**
- âœ… Crossroads analysis (module interaction points)
- âœ… Critical paths (data flow entry points)
- âœ… Architecture overview (top-level structure)

**Medium Priority (Sparse caching):**
- ðŸ“„ Component index (looked up on-demand)
- ðŸ“„ Frequently accessed FQNs (cached as needed)

**Low Priority (File-based lookup):**
- ðŸ“‹ Full component tree (use index + file lookup)
- ðŸ“‹ All integration edges (use crossroads instead)

### Performance Improvement

**Before using these commands:**
- Every question requires file parsing
- Query time: 10-30 seconds per question
- Context underutilized

**After using these commands:**
- Instant answers from memorized crossroads
- Query time: <1 second per question
- Architecture always in focus
- **100-1000x faster** ðŸš€

## Analysis Output Format

Both commands generate analysis in compact format (version 2.0-compact):

```json
{
  "v": "2.0-compact",
  "meta": {
    "c": 12185,                  // Components
    "r": 93355,                  // Relationships/edges
    "ts": "2025-10-17T10:30:00Z"
  },
  "idx": {                        // Component index
    "1": "django",
    "2": "django.http",
    ...
  },
  "cmp": [                        // Flat components
    {"id": 1, "t": "mo", "n": "django", "p": null},
    ...
  ],
  "crd": [                        // Crossroads (module junctions)
    {"id": "django_db_junction", "c": [1, 3], "cnt": 247, "crit": "h"},
    ...
  ],
  "cp": [                         // Critical paths
    {"id": "path_main", "ep": 5, "cc": 88, "cx": "h"},
    ...
  ]
}
```

**Key fields to memorize:**
- `crd`: Crossroads (module interactions)
- `cp`: Critical paths (data flow entry points)
- `idx`: Component lookups (sparse, on-demand)

## Query Examples

Once analysis is loaded in memory, you can ask:

```
"What are the main integration points?"
â†’ Return crossroads (from crd field)

"How does module A connect to module B?"
â†’ Look up crossroads between A and B

"What calls function X?"
â†’ Look up X in index, check its crossroads

"What are the data flow bottlenecks?"
â†’ Return critical paths sorted by call count
```

## Storage & Gitignore

Analysis files are stored locally:
- **Primary:** `.claude/codebase_analysis.json`
- **Backup (optional):** `.claude/codebase_analysis.previous.json`

Add to `.gitignore`:
```
# .gitignore
.claude/codebase_analysis.json
.claude/codebase_analysis.previous.json
```

This keeps analysis local and fresh for each developer.

## Configuration

### Exclude Patterns

By default, both commands exclude:
```
*/tests/*,*/docs/*,*/.git/*,*/__pycache__/*,*/node_modules/*
```

To customize, edit the `--exclude` flags in the command markdown files:

```bash
# Example: Add build directory
--exclude "*/tests/*,*/docs/*,*/.git/*,*/__pycache__/*,*/node_modules/*,*/build/*"
```

### Output Location

By default, analysis is saved to:
```
.claude/codebase_analysis.json
```

This can be customized by editing:
```bash
--output .claude/codebase_analysis.json
```

## Troubleshooting

### "No analysis found"
```
Solution: Run /analyze-python first
```

### "Analysis is too large"
```
Solutions:
1. Exclude more directories
2. Analyze specific module instead of entire project
3. Use on-demand queries instead of memorizing everything
```

### "Analysis is outdated"
```
Solution: Run /refresh-analysis to update
```

### Analysis generation fails
```
Troubleshooting:
1. Check integration_mapper.py exists:
   ls src/integration_mapper/mapper.py

2. Run directly to see error:
   python src/integration_mapper/mapper.py --help

3. Check for syntax errors:
   python -m py_compile changed_file.py
```

## Performance Metrics

### Analysis Generation Time
- 10 files: 1-2 seconds
- 50 files: 3-5 seconds
- 100 files: 5-10 seconds
- 500 files: 15-25 seconds
- 900 files (Django): 20-30 seconds

### Memory Usage
- Small project: 5-15K tokens
- Medium project: 20-40K tokens
- Large project: 60-80K tokens
- Django (900 files): 72.9K tokens

### Context Freed
- Small: 185K-195K tokens
- Medium: 160K-180K tokens
- Large: 120K-140K tokens
- Django: 127.1K tokens for work

## Integration with Workflow

### Typical Session

```
# Start session
/analyze-python

# Work on codebase
# ... make changes ...

# Before commit
/refresh-analysis

# Instant architecture queries
"What did I change in the architecture?"
"Did I create new crossroads?"

# Commit with confidence
```

### With Feature Branches

```
# Main branch analysis
/analyze-python

# Switch to feature branch
git checkout -b feature/new-cache-layer

# Make changes...

# Analyze feature branch
/analyze-python

# Compare architectures
"How does this feature change the integration?"

# Merge back to main
/refresh-analysis  # Refresh main after merge
```

## Advanced Usage

### Multiple Projects

Each project gets its own analysis:
```
ProjectA/.claude/codebase_analysis.json
ProjectB/.claude/codebase_analysis.json
```

Switch between them naturally in your workflow.

### Incremental Development

Use `/refresh-analysis` throughout your session:
```
1. /analyze-python         # Load initial state
2. Make changes
3. /refresh-analysis       # Update after changes
4. Make more changes
5. /refresh-analysis       # Update again
6. Continue with latest understanding
```

### Baseline Comparison

Keep multiple versions:
```bash
# Before major refactoring
cp .claude/codebase_analysis.json \
   .claude/codebase_analysis.baseline.json

# After refactoring
/refresh-analysis

# Compare architectures manually
```

## Summary

These commands transform your development experience:

âœ… **Instant architectural understanding** (no file re-parsing)
âœ… **100-1000x faster** than traditional file parsing
âœ… **Context-aware** (60-70% savings vs verbose)
âœ… **Accurate** (AST-based, not inference)
âœ… **Easy to update** (refresh after changes)

**Result:** Claude Code becomes an architecture-aware coding assistant that never forgets your codebase! ðŸš€
